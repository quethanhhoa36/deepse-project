{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ef7898c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1769148935.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[4], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    pip install tensorflow\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "print(f\"{np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ae8d10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.3\n",
      "1.2.1\n"
     ]
    }
   ],
   "source": [
    "print(f\"{pd.__version__}\")\n",
    "print(f\"{sklearn.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ace657db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import base64\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a03ba77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19 Python files\n",
      "Saved to data/raw/file_0.py\n",
      "Saved to data/raw/file_1.py\n",
      "Saved to data/raw/file_2.py\n",
      "Saved to data/raw/file_3.py\n",
      "Saved to data/raw/file_4.py\n",
      "Saved to data/raw/file_5.py\n",
      "Saved to data/raw/file_6.py\n",
      "Saved to data/raw/file_7.py\n",
      "Saved to data/raw/file_8.py\n",
      "Saved to data/raw/file_9.py\n"
     ]
    }
   ],
   "source": [
    "github_token = \"\" # Tạo token từ GitHub settings\n",
    "headers = {\n",
    "\"Authorization\": f\"token {github_token}\",\n",
    "\"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "# Hàm lấy nội dung file từ repository\n",
    "def get_file_content(owner, repo, path, branch=\"main\"):\n",
    "    url =f\"https://api.github.com/repos/{owner}/{repo}/contents/{path}?ref={branch}\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        content = response.json()\n",
    "# Kiểm tra nếu là \n",
    "        if \"type\" in content and content[\"type\"] == \"file\":\n",
    "            return base64.b64decode(content[\"content\"]).decode(\"utf-8\")\n",
    "    return None\n",
    "    # Lấy danh sách các file Python trong một repository\n",
    "def get_python_files(owner, repo, path=\"\", branch=\"main\"):\n",
    "    url =f\"https://api.github.com/repos/{owner}/{repo}/contents/{path}?ref={branch}\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    files = []\n",
    "    if response.status_code == 200:\n",
    "        contents = response.json()\n",
    "    for item in contents:\n",
    "        if item[\"type\"] == \"file\" and item[\"name\"].endswith(\".py\"):\n",
    "            files.append(item[\"path\"])\n",
    "        elif item[\"type\"] == \"dir\":\n",
    "        # Đệ quy cho thư mục con\n",
    "            files.extend(get_python_files(owner, repo,\n",
    "        item[\"path\"], branch))\n",
    "    return files\n",
    "# Ví dụ sử dụng\n",
    "owner = \"tensorflow\"\n",
    "repo = \"models\"\n",
    "python_files = get_python_files(owner, repo, path=\"official/nlp/modeling/models\",\n",
    "branch=\"master\")\n",
    "print(f\"Found {len(python_files)} Python files\")\n",
    "# Lưu các file vào thư mục local\n",
    "os.makedirs(\"data/raw\", exist_ok=True)\n",
    "for i, file_path in enumerate(python_files[:10]): # Lấy 10 file đầu\n",
    "    content = get_file_content(owner, repo, file_path, branch=\"master\")\n",
    "    if content:\n",
    "        local_path = f\"data/raw/file_{i}.py\"\n",
    "        with open(local_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(content)\n",
    "        print(f\"Saved to {local_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fad2bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 0 functions\n",
      "Extracted 5 functions\n",
      "Extracted 8 functions\n",
      "Extracted 16 functions\n",
      "Extracted 22 functions\n",
      "Extracted 26 functions\n",
      "Extracted 30 functions\n",
      "Extracted 34 functions\n",
      "Extracted 37 functions\n",
      "Extracted 41 functions\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import re\n",
    "def preprocess_python_code(code):\n",
    "    # Loại bỏ comments\n",
    "    code = re.sub(r'#.*', '', code)\n",
    "    code = re.sub(r'\"\"\"[\\s\\S]*?\"\"\"', '', code)\n",
    "    code = re.sub(r\"'''[\\s\\S]*?'''\", '', code)\n",
    "    # Chuẩn hóa khoảng trắng\n",
    "    code = re.sub(r'\\s+', ' ', code)\n",
    "    return code.strip()\n",
    "def extract_functions(code):\n",
    "    try:\n",
    "        tree = ast.parse(code)\n",
    "        functions = []\n",
    "        for node in ast.walk(tree):\n",
    "            if isinstance(node, ast.FunctionDef):\n",
    "                func_code = ast.get_source_segment(code, node)\n",
    "                functions.append({\n",
    "                    'name': node.name,\n",
    "                    'code': func_code,\n",
    "                    'processed_code': preprocess_python_code(func_code)\n",
    "                })\n",
    "        return functions\n",
    "    except SyntaxError:\n",
    "        return []\n",
    "# Ví dụ sử dụng\n",
    "import os\n",
    "processed_data = []\n",
    "for i in range(10): # Cho 10 file đã tải\n",
    "    file_path = f\"data/raw/file_{i}.py\"\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            code = f.read()\n",
    "        # Tiền xử lý và trích xuất hàm\n",
    "        functions = extract_functions(code)\n",
    "        processed_data.extend(functions)\n",
    "        print(f\"Extracted {len(processed_data)} functions\")\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(processed_data)\n",
    "os.makedirs('data/processed')\n",
    "df.to_csv(\"data/processed/functions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bca68964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of TF-IDF matrix: (41, 2217)\n",
      "   100  100 num_layers  100 num_layers bert_trainer_model  \\\n",
      "0  0.0             0.0                                0.0   \n",
      "1  0.0             0.0                                0.0   \n",
      "2  0.0             0.0                                0.0   \n",
      "3  0.0             0.0                                0.0   \n",
      "4  0.0             0.0                                0.0   \n",
      "\n",
      "   100 num_layers cls_head  100 num_layers max_sequence_length  \\\n",
      "0                      0.0                                 0.0   \n",
      "1                      0.0                                 0.0   \n",
      "2                      0.0                                 0.0   \n",
      "3                      0.0                                 0.0   \n",
      "4                      0.0                                 0.0   \n",
      "\n",
      "   100 sequence_length  100 sequence_length 512  100 test_network  \\\n",
      "0                  0.0                      0.0               0.0   \n",
      "1                  0.0                      0.0               0.0   \n",
      "2                  0.0                      0.0               0.0   \n",
      "3                  0.0                      0.0               0.0   \n",
      "4                  0.0                      0.0               0.0   \n",
      "\n",
      "   100 test_network networks   20  ...  word_ids tf_keras  \\\n",
      "0                        0.0  0.0  ...                0.0   \n",
      "1                        0.0  0.0  ...                0.0   \n",
      "2                        0.0  0.0  ...                0.0   \n",
      "3                        0.0  0.0  ...                0.0   \n",
      "4                        0.0  0.0  ...                0.0   \n",
      "\n",
      "   word_ids tf_keras input  zeros  zeros cls_head  zeros cls_head cls_head  \\\n",
      "0                      0.0    0.0             0.0                      0.0   \n",
      "1                      0.0    0.0             0.0                      0.0   \n",
      "2                      0.0    0.0             0.0                      0.0   \n",
      "3                      0.0    0.0             0.0                      0.0   \n",
      "4                      0.0    0.0             0.0                      0.0   \n",
      "\n",
      "   zeros output  zeros output predictions  zip  zip self  zip self inputs  \n",
      "0           0.0                       0.0  0.0       0.0              0.0  \n",
      "1           0.0                       0.0  0.0       0.0              0.0  \n",
      "2           0.0                       0.0  0.0       0.0              0.0  \n",
      "3           0.0                       0.0  0.0       0.0              0.0  \n",
      "4           0.0                       0.0  0.0       0.0              0.0  \n",
      "\n",
      "[5 rows x 2217 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Đọc dữ liệu đã xử lý\n",
    "df = pd.read_csv(\"data/processed/functions.csv\")\n",
    "# Khởi tạo TF-IDF vectorizer\n",
    "tfidf = TfidfVectorizer(\n",
    "max_features=5000, # Giới hạn số lượng từ\n",
    "ngram_range=(1, 3), # Sử dụng unigram, bigram và trigram\n",
    "stop_words='english' # Loại bỏ stopwords\n",
    ")\n",
    "# Tạo ma trận TF-IDF\n",
    "tfidf_matrix = tfidf.fit_transform(df['processed_code'])\n",
    "# Chuyển ma trận thành DataFrame để dễ xem\n",
    "tfidf_df = pd.DataFrame(\n",
    "tfidf_matrix.toarray(),\n",
    "columns=tfidf.get_feature_names_out()\n",
    ")\n",
    "print(f\"Shape of TF-IDF matrix: {tfidf_matrix.shape}\")\n",
    "print(tfidf_df.head())\n",
    "# Lưu vectorizer để tái sử dụng\n",
    "import pickle\n",
    "os.makedirs('models')\n",
    "with open(\"models/tfidf_vectorizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tfidf, f)\n",
    "import numpy as np\n",
    "np.save(\"data/processed/tfidf_matrix.npy\", tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b3109d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\student\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Word2Vec embeddings: (41, 100)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Tải tokenizer\n",
    "nltk.download('punkt')\n",
    "# Chuẩn bị dữ liệu cho Word2Vec\n",
    "tokenized_code = []\n",
    "for code in df['processed_code']:\n",
    "    tokens = word_tokenize(code)\n",
    "    tokenized_code.append(tokens)\n",
    "# Huấn luyện mô hình Word2Vec\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=tokenized_code,\n",
    "    vector_size=100, # Kích thước vector\n",
    "    window=5, # Kích thước cửa sổ ngữ cảnh\n",
    "    min_count=2, # Tối thiểu số lần xuất hiện của từ\n",
    "    workers=4 # Số luồng\n",
    ")\n",
    "# Lưu mô hình\n",
    "w2v_model.save(\"models/w2v_code.model\")\n",
    "# Tạo embedding cho mỗi hàm bằng cách lấy trung bình các vector từ\n",
    "def create_document_vector(doc_tokens, model):\n",
    "    doc_vector = []\n",
    "    for token in doc_tokens:\n",
    "        if token in model.wv:\n",
    "            doc_vector.append(model.wv[token])\n",
    "    if not doc_vector:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(doc_vector, axis=0)\n",
    "# Tạo embedding cho mỗi hàm\n",
    "doc_vectors = []\n",
    "for tokens in tokenized_code:\n",
    "    doc_vectors.append(create_document_vector(tokens, w2v_model))\n",
    "doc_vectors_array = np.array(doc_vectors)\n",
    "np.save(\"data/processed/w2v_vectors.npy\", doc_vectors_array)\n",
    "print(f\"Shape of Word2Vec embeddings: {doc_vectors_array.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c04c3209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Đọc dữ liệu vector\n",
    "tfidf_vectors = np.load(\"data/processed/tfidf_matrix.npy\")\n",
    "w2v_vectors = np.load(\"data/processed/w2v_vectors.npy\")\n",
    "# Giả định: Gán nhãn cho mỗi hàm (ví dụ: phân loại theo chức năng)\n",
    "# Trong thực tế, bạn cần có dữ liệu đã được gán nhãn\n",
    "# Ở đây, chúng ta tạo nhãn giả cho mục đích demo\n",
    "df = pd.read_csv(\"data/processed/functions.csv\")\n",
    "# Ví dụ: Phân loại hàm theo tên\n",
    "# 0: hàm bắt đầu bằng \"get_\" hoặc \"fetch_\"\n",
    "# 1: hàm bắt đầu bằng \"create_\" hoặc \"build_\"\n",
    "# 2: các hàm còn lại\n",
    "def assign_label(func_name):\n",
    "    if func_name.startswith(('get_', 'fetch_')):\n",
    "        return 0\n",
    "    elif func_name.startswith(('create_', 'build_')):\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "df['label'] = df['name'].apply(assign_label)\n",
    "# Chia dữ liệu\n",
    "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(\n",
    "tfidf_vectors, df['label'], test_size=0.3, random_state=42\n",
    ")\n",
    "X_train_w2v, X_test_w2v, _, _ = train_test_split(\n",
    "w2v_vectors, df['label'], test_size=0.3, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2f045e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM với TF-IDF:\n",
      "Accuracy: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         1\n",
      "           2       1.00      1.00      1.00        12\n",
      "\n",
      "    accuracy                           1.00        13\n",
      "   macro avg       1.00      1.00      1.00        13\n",
      "weighted avg       1.00      1.00      1.00        13\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "# Mô hình SVM\n",
    "svm_model = SVC(kernel='linear', C=1.0, random_state=42)\n",
    "svm_model.fit(X_train_tfidf, y_train)\n",
    "# Dự đoán\n",
    "y_pred = svm_model.predict(X_test_tfidf)\n",
    "# Đánh giá\n",
    "print(\"SVM với TF-IDF:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "# Lưu mô hình\n",
    "import pickle\n",
    "with open(\"models/svm_tfidf.pkl\", \"wb\") as f:\n",
    "    pickle.dump(svm_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "19b4c84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest với Word2Vec:\n",
      "Accuracy: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         1\n",
      "           2       1.00      1.00      1.00        12\n",
      "\n",
      "    accuracy                           1.00        13\n",
      "   macro avg       1.00      1.00      1.00        13\n",
      "weighted avg       1.00      1.00      1.00        13\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Mô hình Random Forest\n",
    "rf_model = RandomForestClassifier(\n",
    "n_estimators=100,\n",
    "max_depth=10,\n",
    "random_state=42\n",
    ")\n",
    "rf_model.fit(X_train_w2v, y_train)\n",
    "# Dự đoán\n",
    "y_pred = rf_model.predict(X_test_w2v)\n",
    "# Đánh giá\n",
    "print(\"\\nRandom Forest với Word2Vec:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "# Lưu mô hình\n",
    "with open(\"models/rf_w2v.pkl\", \"wb\") as f:\n",
    "    pickle.dump(rf_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "575aee07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest với TF-IDF:\n",
      "Accuracy: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         1\n",
      "           2       1.00      1.00      1.00        12\n",
      "\n",
      "    accuracy                           1.00        13\n",
      "   macro avg       1.00      1.00      1.00        13\n",
      "weighted avg       1.00      1.00      1.00        13\n",
      "\n",
      "\n",
      "So sánh biểu diễn vector:\n",
      "SVM + TF-IDF: 1.0000\n",
      "RF + Word2Vec: 1.0000\n",
      "RF + TF-IDF: 1.0000\n"
     ]
    }
   ],
   "source": [
    "rf_tfidf = RandomForestClassifier(\n",
    "n_estimators=100,\n",
    "max_depth=10,\n",
    "random_state=42\n",
    ")\n",
    "rf_tfidf.fit(X_train_tfidf, y_train)\n",
    "y_pred_tfidf = rf_tfidf.predict(X_test_tfidf)\n",
    "# Đánh giá\n",
    "print(\"\\nRandom Forest với TF-IDF:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_tfidf):.4f}\")\n",
    "print(classification_report(y_test, y_pred_tfidf))\n",
    "# Kết luận\n",
    "print(\"\\nSo sánh biểu diễn vector:\")\n",
    "print(f\"SVM + TF-IDF: {accuracy_score(y_test,svm_model.predict(X_test_tfidf)):.4f}\")\n",
    "print(f\"RF + Word2Vec: {accuracy_score(y_test,rf_model.predict(X_test_w2v)):.4f}\")\n",
    "print(f\"RF + TF-IDF: {accuracy_score(y_test, y_pred_tfidf):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d448a7ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
